{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from GAN import GAN\n",
    "import numpy as np\n",
    "import os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with WGAN and mnist results\n",
    "I tried different options with watered down but result images weren't good</br>\n",
    "<ol>\n",
    "    <li> WGAN GP takes a long time to train\n",
    "    <li> WGAN does not create good images at short period of times\n",
    "   \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img, code):\n",
    "    if code == 0:\n",
    "        return(img)\n",
    "    \n",
    "    if code == 1:\n",
    "        ang = np.random.randint(low=-10, high=10)\n",
    "        p = np.int(np.ceil(np.sin(np.deg2rad(np.abs(ang)))*img.shape[0]))\n",
    "        img = rotate(img, ang)\n",
    "        img[:, 0:p] = -1\n",
    "        img[:,-p::] = -1\n",
    "        img[0:p, :] = -1\n",
    "        img[-p::, :] = -1\n",
    "        \n",
    "        return(img)\n",
    "    \n",
    "    \n",
    "    if code == 2:\n",
    "        sft = np.random.randint(-4, 4)\n",
    "        tr = SimilarityTransform(scale=1, rotation=0, translation=[sft,sft])\n",
    "        img = warp(img, tr)\n",
    "        if sft>0:\n",
    "            img[:,-sft::] = -1\n",
    "            img[-sft::,:] = -1\n",
    "        else:\n",
    "            img[:,0:-sft] = -1\n",
    "            img[0:-sft,:] = -1\n",
    "        return(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No TRnsform\n",
    "mn = mnist.Mnist(path=\".\")\n",
    "imgs, lbls, _ = mn.get_batch(60000)\n",
    "imgs = (imgs - 0.5)*2\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    global imgs\n",
    "    rnd_idxs = np.random.choice(np.arange(imgs.shape[0]), batch_size)\n",
    "    #for idx in rnd_idxs:\n",
    "        #code = np.random.choice(np.arange(3), 1)[0]\n",
    "        #imgs[idx] = transform_image(imgs[idx], code=code)\n",
    "    return (imgs[rnd_idxs])    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN different settig mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_op=[0, 1, 2, 3]\n",
    "mb_op =[[4, 5], [2, 3]]\n",
    "\n",
    "def get_pars():\n",
    "    return(ns_op[np.random.randint(0, len(ns_op))], np.random.randint(100, 500), np.round(np.random.uniform(0.8, 0.92), 2), mb_op[np.random.randint(0, len(mb_op))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_settings():\n",
    "    z_dim = 100\n",
    "    tf.reset_default_graph()\n",
    "    gan = GAN(verbose=True)\n",
    "    layers = list()\n",
    "    nsl, dec, sm, mba = get_pars()\n",
    "    cases.append((nsl, dec, sm, mba))\n",
    "    ns=np.zeros(3, int).tolist()\n",
    "    if nsl>0:\n",
    "        for i in range(nsl):\n",
    "            ns[i] = tf.train.exponential_decay(0.1, tf.Variable(0, trainable=False), dec, 0.95, staircase=True)\n",
    "\n",
    "    G_layers = [[1024, [4, 4], (1, 1), \"valid\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [512, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [256, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [128, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [1, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 0, \"th\", None, 0]]\n",
    "\n",
    "    D_layers = [[128, [4, 4], (2, 2), 'same', tf.random_normal_initializer(stddev=0.02), 0, 'lr', None,  ns[0]],\n",
    "                [256, [4, 4], (2, 2), 'same', tf.random_normal_initializer(stddev=0.02), 1, 'lr', None, ns[1]],\n",
    "                [512, [4, 4], (2, 2), 'same', tf.random_normal_initializer(stddev=0.02), 1, 'lr', None, ns[2]],\n",
    "                [1024, [4, 4], (2, 2), 'same', tf.random_normal_initializer(stddev=0.02), 1, 'lr', mba, 0],\n",
    "                [1, [4, 4], (1, 1), 'valid', tf.random_normal_initializer(stddev=0.02), 0, 'l', None, 0]]\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "    xr = tf.image.resize_images(tf.expand_dims(x, axis=3), (64, 64))\n",
    "\n",
    "    gargs = gan.build_graph(G_layers, D_layers, xr,\n",
    "                           tf.placeholder(tf.float32, [None, 1, 1, z_dim]), True, sm, [\"RMS\"], 2e-4, True)\n",
    "\n",
    "    gargs = [g for g in gargs]\n",
    "    gargs[1] = x\n",
    "    gargs = tuple(gargs)\n",
    "\n",
    "    gan.train(60,  3000, 10, get_batch, \"n\", z_dim, True, \"summary\\\\GAN_mnist_case_dn_{}\".format(len(cases)), \"Model\\\\GAN_mnist_case_dn_{}\".format(len(cases)), None, None, gargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(8):\n",
    "    try_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"GAN_cases_dn.npz\", [str(c) for c in cases])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"GAN_cases_dn.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(save_path):\n",
    "    z_dim=100\n",
    "    G_layers = [[1024, [4, 4], (1, 1), \"valid\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [512, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [256, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [128, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 1, \"r\", None, 0],\n",
    "                [1, [4, 4], (2, 2), \"same\", tf.random_normal_initializer(stddev=0.02), 0, \"th\", None, 0]]\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gan = GAN(verbose=False)\n",
    "    z = tf.placeholder(shape=[20, 1, 1, z_dim], dtype=tf.float32)\n",
    "    graph, sess, z, G_col, G_out = gan.build_inference_graph(G_layers, z, save_path)\n",
    "    z_in = gan._get_z_code(20, \"n\", z_dim)\n",
    "    images = sess.run(G_out, feed_dict={z:z_in})\n",
    "    sess.close()\n",
    "    return (images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for i in range(1,9):\n",
    "    img = np.clip(generate_img(\"Model\\\\GAN_mnist_case_dn_{}-2999\".format(i)).squeeze()/2+0.5, 0, 1)\n",
    "    img_list.append(img)\n",
    "\n",
    "fig, ax = plt.subplots(4, 2)\n",
    "fig.set_size_inches(15, 25)\n",
    "for i, img in enumerate(img_list):\n",
    "    ax[i//2,i-2*(i//2)].set_title(i)\n",
    "    ax[i//2,i-2*(i//2)].imshow(np.vstack([np.hstack(img[0:5,...]), np.hstack(img[5:10,...]),\n",
    "                     np.hstack(img[10:15,...]), np.hstack(img[15:20,...])]), cmap=\"gray\") \n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good settings on mnist\n",
    "<table align=\"center\">\n",
    "    <tr>\n",
    "        <th>layers with noise</th>\n",
    "        <th>decay steps of 3000 epochs</th>\n",
    "        <th>smoothing</th>\n",
    "        <th>mini-batch discr.</th>\n",
    "    </tr>\n",
    "    <tr><td>3</td><td>465</td><td>0.91</td><td>[2, 3]</td></tr>\n",
    "    <tr><td>3</td><td>390</td><td>0.85</td><td>[2, 3]</td></tr>\n",
    "    <tr><td>3</td><td>440</td><td>0.87</td><td>[2, 3]</td></tr>\n",
    "    <tr><td>3</td><td>489</td><td>0.82</td><td>[2, 3]</td></tr>\n",
    "   \n",
    "</table>\n",
    "\n",
    "<ol>\n",
    "    <li>so adding noise to all discriminator layers are good option\n",
    "    <li>also decay of 14% of total epochs are good like 489 is better that 390\n",
    "\n",
    "</ol></p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
